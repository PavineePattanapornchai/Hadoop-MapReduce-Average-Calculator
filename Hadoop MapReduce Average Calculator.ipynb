{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DPeruWAMOpL-"
   },
   "source": [
    "Hadoop MapReduce\n",
    "\n",
    "Write Python programs as a mapper and a reducer to find the avearage value from a given text file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wcFJa81DS-t0"
   },
   "source": [
    "## Download and Install Hadoop\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K2zeI4pWBHrE",
    "outputId": "3146a292-7480-4c04-8647-85f86a100c5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-10-30 09:41:18--  https://dlcdn.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz\n",
      "Resolving dlcdn.apache.org (dlcdn.apache.org)... 151.101.2.132, 2a04:4e42::644\n",
      "Connecting to dlcdn.apache.org (dlcdn.apache.org)|151.101.2.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 730107476 (696M) [application/x-gzip]\n",
      "Saving to: ‘hadoop-3.3.6.tar.gz’\n",
      "\n",
      "hadoop-3.3.6.tar.gz 100%[===================>] 696.28M  75.0MB/s    in 11s     \n",
      "\n",
      "2024-10-30 09:41:29 (66.0 MB/s) - ‘hadoop-3.3.6.tar.gz’ saved [730107476/730107476]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://dlcdn.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "gss5h5FHM8XY"
   },
   "outputs": [],
   "source": [
    "!tar zxf hadoop-3.3.6.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "z4EGe77vNAWH"
   },
   "outputs": [],
   "source": [
    "!mv hadoop-3.3.6 /usr/local/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UrojxvrjR1CV"
   },
   "source": [
    "## Set JAVA_HOME and PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "AWZWbl-bOJE7"
   },
   "outputs": [],
   "source": [
    "java_home = !readlink -f /usr/bin/java | sed \"s:bin/java::\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "6Cxwp3LpSUij"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['JAVA_HOME'] = java_home[0]\n",
    "os.environ['PATH'] += ':/usr/local/hadoop-3.3.6/bin'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VhYNIUI8RdGx"
   },
   "source": [
    "## Update the following Mapper and Reduce to find the avarage value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iKms9RabTgRt",
    "outputId": "bc7ff2cd-c657-45aa-fef6-84043974bbef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test.dat\n"
     ]
    }
   ],
   "source": [
    "%%file test.dat\n",
    "12 32 56 33 45 76 11 232 53 11 23\n",
    "22 65 13\n",
    "55 867 323\n",
    "1123 343\n",
    "112\n",
    "31 11 22 33 44 55 66 77 87 445 55 23\n",
    "112 233 44 33 44 55 66 33 44 55 676"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mfCbIqyBRVOR",
    "outputId": "f8621168-eb6f-4801-83cf-d85d679dc095"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing average_mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%file average_mapper.py\n",
    "\n",
    "import sys\n",
    "import io\n",
    "\n",
    "input_stream = io.TextIOWrapper(sys.stdin.buffer, encoding='utf8')\n",
    "sum_values = 0\n",
    "count = 0\n",
    "for line in input_stream:\n",
    "    line = line.strip()\n",
    "    strs = line.split()\n",
    "    nums = [int(x) for x in strs if x.isnumeric()]\n",
    "    sum_values += sum(nums)\n",
    "    count += len(nums)\n",
    "\n",
    "print(f\"1\\t{sum_values},{count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZnjrRD6DS8t5",
    "outputId": "95272948-d94e-4cc4-c41f-baea34ae5b65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing average_reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%file average_reducer.py\n",
    "\n",
    "import math\n",
    "import sys\n",
    "\n",
    "total_sum = 0\n",
    "total_count = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    key, value = line.split(\"\\t\", 1)\n",
    "    sum_value, count = value.split(\",\")\n",
    "    sum_value = int(sum_value)\n",
    "    count = int(count)\n",
    "    total_sum += sum_value\n",
    "    total_count += count\n",
    "\n",
    "average = total_sum / total_count\n",
    "print(average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "WRvCLjxXULdn"
   },
   "outputs": [],
   "source": [
    "!rm -rf /content/average_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dFlIo4cqTwgr",
    "outputId": "628b1eef-09e0-489c-808b-6e7b8e4ebd51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-30 09:43:48,244 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [/content/average_mapper.py, /content/average_reducer.py] [] /tmp/streamjob15255059489415559442.jar tmpDir=null\n",
      "2024-10-30 09:43:49,218 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
      "2024-10-30 09:43:49,464 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "2024-10-30 09:43:49,465 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
      "2024-10-30 09:43:49,488 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
      "2024-10-30 09:43:49,802 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2024-10-30 09:43:49,827 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "2024-10-30 09:43:50,293 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1487849564_0001\n",
      "2024-10-30 09:43:50,293 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2024-10-30 09:43:50,734 INFO mapred.LocalDistributedCacheManager: Localized file:/content/average_mapper.py as file:/tmp/hadoop-root/mapred/local/job_local1487849564_0001_c67f83e0-c5f9-499c-ae05-e6636a10b5a3/average_mapper.py\n",
      "2024-10-30 09:43:50,772 INFO mapred.LocalDistributedCacheManager: Localized file:/content/average_reducer.py as file:/tmp/hadoop-root/mapred/local/job_local1487849564_0001_3107e8b5-9f74-41a4-b4a8-74faaa3f58e3/average_reducer.py\n",
      "2024-10-30 09:43:50,883 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "2024-10-30 09:43:50,885 INFO mapreduce.Job: Running job: job_local1487849564_0001\n",
      "2024-10-30 09:43:50,891 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "2024-10-30 09:43:50,894 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "2024-10-30 09:43:50,900 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "2024-10-30 09:43:50,900 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2024-10-30 09:43:50,949 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "2024-10-30 09:43:50,953 INFO mapred.LocalJobRunner: Starting task: attempt_local1487849564_0001_m_000000_0\n",
      "2024-10-30 09:43:51,002 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "2024-10-30 09:43:51,002 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2024-10-30 09:43:51,033 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "2024-10-30 09:43:51,049 INFO mapred.MapTask: Processing split: file:/content/test.dat:0+140\n",
      "2024-10-30 09:43:51,070 INFO mapred.MapTask: numReduceTasks: 1\n",
      "2024-10-30 09:43:51,163 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "2024-10-30 09:43:51,163 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "2024-10-30 09:43:51,163 INFO mapred.MapTask: soft limit at 83886080\n",
      "2024-10-30 09:43:51,163 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "2024-10-30 09:43:51,163 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "2024-10-30 09:43:51,167 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "2024-10-30 09:43:51,178 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, average_mapper.py]\n",
      "2024-10-30 09:43:51,183 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "2024-10-30 09:43:51,186 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "2024-10-30 09:43:51,186 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "2024-10-30 09:43:51,187 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "2024-10-30 09:43:51,188 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "2024-10-30 09:43:51,189 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "2024-10-30 09:43:51,192 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "2024-10-30 09:43:51,193 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "2024-10-30 09:43:51,194 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "2024-10-30 09:43:51,194 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "2024-10-30 09:43:51,195 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "2024-10-30 09:43:51,197 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "2024-10-30 09:43:51,228 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2024-10-30 09:43:51,286 INFO streaming.PipeMapRed: Records R/W=7/1\n",
      "2024-10-30 09:43:51,291 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "2024-10-30 09:43:51,292 INFO streaming.PipeMapRed: mapRedFinished\n",
      "2024-10-30 09:43:51,297 INFO mapred.LocalJobRunner: \n",
      "2024-10-30 09:43:51,297 INFO mapred.MapTask: Starting flush of map output\n",
      "2024-10-30 09:43:51,297 INFO mapred.MapTask: Spilling map output\n",
      "2024-10-30 09:43:51,297 INFO mapred.MapTask: bufstart = 0; bufend = 10; bufvoid = 104857600\n",
      "2024-10-30 09:43:51,297 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214396(104857584); length = 1/6553600\n",
      "2024-10-30 09:43:51,307 INFO mapred.MapTask: Finished spill 0\n",
      "2024-10-30 09:43:51,323 INFO mapred.Task: Task:attempt_local1487849564_0001_m_000000_0 is done. And is in the process of committing\n",
      "2024-10-30 09:43:51,329 INFO mapred.LocalJobRunner: Records R/W=7/1\n",
      "2024-10-30 09:43:51,329 INFO mapred.Task: Task 'attempt_local1487849564_0001_m_000000_0' done.\n",
      "2024-10-30 09:43:51,339 INFO mapred.Task: Final Counters for attempt_local1487849564_0001_m_000000_0: Counters: 17\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1585\n",
      "\t\tFILE: Number of bytes written=644802\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=7\n",
      "\t\tMap output records=1\n",
      "\t\tMap output bytes=10\n",
      "\t\tMap output materialized bytes=18\n",
      "\t\tInput split bytes=74\n",
      "\t\tCombine input records=0\n",
      "\t\tSpilled Records=1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=307232768\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=140\n",
      "2024-10-30 09:43:51,339 INFO mapred.LocalJobRunner: Finishing task: attempt_local1487849564_0001_m_000000_0\n",
      "2024-10-30 09:43:51,339 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "2024-10-30 09:43:51,351 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "2024-10-30 09:43:51,352 INFO mapred.LocalJobRunner: Starting task: attempt_local1487849564_0001_r_000000_0\n",
      "2024-10-30 09:43:51,366 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "2024-10-30 09:43:51,366 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2024-10-30 09:43:51,366 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "2024-10-30 09:43:51,376 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@5e9c0350\n",
      "2024-10-30 09:43:51,379 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
      "2024-10-30 09:43:51,415 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "2024-10-30 09:43:51,425 INFO reduce.EventFetcher: attempt_local1487849564_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "2024-10-30 09:43:51,470 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1487849564_0001_m_000000_0 decomp: 14 len: 18 to MEMORY\n",
      "2024-10-30 09:43:51,474 INFO reduce.InMemoryMapOutput: Read 14 bytes from map-output for attempt_local1487849564_0001_m_000000_0\n",
      "2024-10-30 09:43:51,476 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 14, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->14\n",
      "2024-10-30 09:43:51,483 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "2024-10-30 09:43:51,494 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "2024-10-30 09:43:51,496 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "2024-10-30 09:43:51,504 INFO mapred.Merger: Merging 1 sorted segments\n",
      "2024-10-30 09:43:51,504 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 10 bytes\n",
      "2024-10-30 09:43:51,506 INFO reduce.MergeManagerImpl: Merged 1 segments, 14 bytes to disk to satisfy reduce memory limit\n",
      "2024-10-30 09:43:51,507 INFO reduce.MergeManagerImpl: Merging 1 files, 18 bytes from disk\n",
      "2024-10-30 09:43:51,507 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "2024-10-30 09:43:51,508 INFO mapred.Merger: Merging 1 sorted segments\n",
      "2024-10-30 09:43:51,509 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 10 bytes\n",
      "2024-10-30 09:43:51,510 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "2024-10-30 09:43:51,521 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, average_reducer.py]\n",
      "2024-10-30 09:43:51,526 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "2024-10-30 09:43:51,530 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "2024-10-30 09:43:51,559 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "2024-10-30 09:43:51,650 INFO streaming.PipeMapRed: Records R/W=1/1\n",
      "2024-10-30 09:43:51,653 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "2024-10-30 09:43:51,654 INFO streaming.PipeMapRed: mapRedFinished\n",
      "2024-10-30 09:43:51,656 INFO mapred.Task: Task:attempt_local1487849564_0001_r_000000_0 is done. And is in the process of committing\n",
      "2024-10-30 09:43:51,657 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
      "2024-10-30 09:43:51,657 INFO mapred.Task: Task attempt_local1487849564_0001_r_000000_0 is allowed to commit now\n",
      "2024-10-30 09:43:51,659 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1487849564_0001_r_000000_0' to file:/content/average_output\n",
      "2024-10-30 09:43:51,661 INFO mapred.LocalJobRunner: Records R/W=1/1 > reduce\n",
      "2024-10-30 09:43:51,661 INFO mapred.Task: Task 'attempt_local1487849564_0001_r_000000_0' done.\n",
      "2024-10-30 09:43:51,661 INFO mapred.Task: Final Counters for attempt_local1487849564_0001_r_000000_0: Counters: 24\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1653\n",
      "\t\tFILE: Number of bytes written=644852\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce shuffle bytes=18\n",
      "\t\tReduce input records=1\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=1\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=307232768\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=32\n",
      "2024-10-30 09:43:51,662 INFO mapred.LocalJobRunner: Finishing task: attempt_local1487849564_0001_r_000000_0\n",
      "2024-10-30 09:43:51,662 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "2024-10-30 09:43:51,890 INFO mapreduce.Job: Job job_local1487849564_0001 running in uber mode : false\n",
      "2024-10-30 09:43:51,892 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2024-10-30 09:43:51,893 INFO mapreduce.Job: Job job_local1487849564_0001 completed successfully\n",
      "2024-10-30 09:43:51,905 INFO mapreduce.Job: Counters: 30\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=3238\n",
      "\t\tFILE: Number of bytes written=1289654\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=7\n",
      "\t\tMap output records=1\n",
      "\t\tMap output bytes=10\n",
      "\t\tMap output materialized bytes=18\n",
      "\t\tInput split bytes=74\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce shuffle bytes=18\n",
      "\t\tReduce input records=1\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=2\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=614465536\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=140\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=32\n",
      "2024-10-30 09:43:51,905 INFO streaming.StreamJob: Output directory: /content/average_output\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/hadoop-3.3.6/bin/hadoop jar /usr/local/hadoop-3.3.6/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar -input /content/test.dat -output /content/average_output -file /content/average_mapper.py -file /content/average_reducer.py  -mapper 'python average_mapper.py'  -reducer 'python average_reducer.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qOUBwtkzUamp",
    "outputId": "b1a0d375-9e62-4cf4-a5a6-15f64c318a95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00000  _SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!ls /content/average_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-GGUu6B9Ud9-",
    "outputId": "8f8b582d-dd73-44e5-bc82-4477d41ddf85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136.06976744186048\t\n"
     ]
    }
   ],
   "source": [
    "!cat /content/average_output/part-00000"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
